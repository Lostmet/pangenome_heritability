import os
import glob
import re
import pandas as pd
import numpy as np
from tqdm import tqdm
from pathlib import Path
from concurrent.futures import ProcessPoolExecutor, as_completed
from typing import List, Dict, Tuple, Optional
from dataclasses import dataclass
from collections import defaultdict
from itertools import combinations
from ..utils.logging_utils import get_logger
logger = get_logger(__name__)


def parse_fasta_with_metadata(file_path: str):
    """
    Parse the given Fasta file, which is formatted like:
    
    >Group_2_2_50381
    C
    >Variant_2_2_50381_50381
    CTATGACTTTTGAATAAAAACCTAAAG
    >Variant_2_2_...
    ...
    >Group_2_3
    AA...
    >Variant_2_3_...

    Returns a data structure:
    {
      "Group_2_2_50381": {
          "reference": "C",
          "variants": [
              {
                  "chrom": 2,
                  "group": 2,
                  "start": 50381,
                  "end": 50381,
                  "sequence": "CTATGACTTTTGAATAAAAACCTAAAG"
              },
              ...
          ]
      },
      ...
    }
    """
    data = {}
    current_group = None
    current_reference = None

    with open(file_path, 'r') as f:
        for line in f:
            line = line.strip()
            if not line:
                continue

            if line.startswith(">Group_"):
                # Parse group header, e.g., "Group_2_2"
                # Extract group identifier
                current_group = line[1:]  # Remove '>'
                data[current_group] = {
                    "reference": "",
                    "variants": []
                }
                current_reference = True  # The next line is the reference for this group
            elif line.startswith(">Variant_"):
                # If a Variant is encountered, parse its coordinate information
                variant_header = line[1:]  # e.g., "Variant_2_2_50381_50381"
                chrom, grp, start, end = parse_variant_header(variant_header)
                # Read the next line (or lines) as the sequence
                variant_seq = next(f).strip()  # Assume the variant always occupies one line

                if current_group is None:
                    raise ValueError("Found a Variant line before any Group line")

                data[current_group]["variants"].append({
                    "chrom": chrom,
                    "group": grp,
                    "start": start,
                    "end": end,
                    "sequence": variant_seq
                })
                current_reference = False
            else:
                # If it's neither a Group header nor a Variant header,
                # and current_reference=True, then this is the group's reference sequence
                if current_group and current_reference:
                    data[current_group]["reference"] += line
                else:
                    
                    pass
    #print(f"meta data: {data}")
    return data


def parse_variant_header(header: str):
    """
    Parse Variant_2_2_50381_50381
    Return (chrom, group, start, end)
    """
    # Assume the format is always correct: Variant_<chrom>_<group>_<start>_<end>
    pattern = r"^Variant_(\d+)_(\d+)_(\d+)_(\d+)$"
    match = re.match(pattern, header)
    if not match:
        raise ValueError(f"Variant header not in expected format: {header}")
    chrom = int(match.group(1))
    grp = int(match.group(2))
    start = int(match.group(3))
    end = int(match.group(4))
    return chrom, grp, start, end
def read_fasta_files(directory: str) -> Dict[str, List[Tuple[str, str]]]:
    """Read aligned FASTA files and handle missing/empty files."""
    pattern = os.path.join(directory, 'Group_*_*_aligned.fasta')
    file_paths = glob.glob(pattern)
    file_paths.sort()
    logger.info(f"Found {len(file_paths)} files matching the pattern.")
    
    fasta_contents = {}
    
    for file_path in file_paths:
        try:
            sequences = []
            with open(file_path, 'r') as file:
                current_seq = []
                seq_id = ''
                
                for line in file:
                    line = line.strip()
                    if line.startswith('>'):
                        if seq_id and current_seq:
                            sequences.append((seq_id, ''.join(current_seq)))
                        seq_id = line[1:]
                        current_seq = []
                    else:
                        current_seq.append(line)
                
                if seq_id and current_seq:
                    sequences.append((seq_id, ''.join(current_seq)))
            
            # blank file
            if not sequences:
                logger.warning(f"Empty aligned file detected: {file_path}")
                fasta_contents[os.path.basename(file_path)] = [('seq0', ''), ('seq1', '')]
            else:
                fasta_contents[os.path.basename(file_path)] = sequences
        
        except Exception as e:
            logger.error(f"Error processing file {file_path}: {str(e)}")
    
    return fasta_contents


def rSV_window_meta(sequence: str, start_offset: int) -> list:
    """
    Given a sequence and its start coordinate in the genome (start_offset),
    return [{pos: ..., rSV: ...}, ...], where pos is the absolute coordinate.
    """
    windows = []
    for i in range(len(sequence)):
        windows.append({
            'pos': start_offset + i,  # Calculate absolute coordinate
            'rSV': sequence[i:i + 1]
        })
    return windows


### <-- NEW OR MODIFIED CODE ###
def compare_rSVs_with_meta(ref_windows: List[Dict], var_windows: List[Dict]) -> Tuple[List[int], List[Dict]]:
    """
    Compare K-mer windows, treating '-' as a normal base for comparison.
    Deletion marks are only handled in the subsequent merging stage.
    
    Args:
        ref_windows: List of k-mer windows for the reference sequence
        var_windows: List of k-mer windows for the variant sequence
    
    Returns:
        Tuple[List[int], List[Dict]]: (difference array, metadata array)
    """
    if not ref_windows or not var_windows:
        logger.warning("Input windows are empty")
        return [0], [{'pos': 0, 'ref': '', 'alt': ''}]

    length = min(len(ref_windows), len(var_windows))
    
    diff_array = []
    meta_array = []

    for i in range(length): # æ¢å¤å¯¹ç¬¬ä¸€ä¸ªçš„æ¯”å¯¹
        ref = ref_windows[i]
        var = var_windows[i]
        
        
        diff_val = 0 if ref["rSV"] == var["rSV"] else 1
        diff_array.append(diff_val)
        
        
        meta_array.append({
            'pos': ref["pos"],
            'ref': ref["rSV"],
            'alt': var["rSV"]
        })

    return diff_array, meta_array


def rSV_window(sequence: str, k: int = 4) -> List[str]:
    """
    Original simple version (if only a pure string list is needed).
    If rSV_window_meta is used, this function may no longer be needed.
    """
    return [sequence[i:i + k] for i in range(len(sequence) - k + 1)]


def compare_windows(ref_windows, var_windows):
    """
    Original compare_windows function.
    If compare_rSVs_with_meta is used, this function may no longer be needed.
    """
    if len(ref_windows) != len(var_windows):
        raise ValueError("Reference and variant windows must have the same length.")
    return [0 if ref == var else 1 for ref, var in zip(ref_windows, var_windows)]


def process_sequences(file_name: str, sequences: List[Tuple[str, str]], genome_metadata: dict ,has_insertion_dict: Dict) -> Dict:
    """
    Process sequence data, including simple seq0/seq1 cases and complex multi-sequence cases.
    """
    try:
       
        group_name = file_name.replace('_aligned.fasta', '').replace('_input.fasta', '')

        group_metadata = genome_metadata.get(group_name)
        #print(f"group_metadata: {group_metadata}")
        if not group_metadata:
            raise ValueError(f"Group {group_name} not found in genome metadata")
        
        results = []
        reference_seq = None
        reference_id = None            
    
        for seq_id, sequence in sequences:
            if seq_id.startswith('ref_') or seq_id == 'reference':
                reference_seq = sequence
                reference_id = seq_id
                break            
        
        if not reference_seq:
            reference_seq = sequences[0][1] 
            reference_id = sequences[0][0]            
        
        start_pos = group_metadata["variants"][0]["start"] if group_metadata["variants"] else 0
        
        # ref_windowsï¼š[{'pos': 906668, 'rSV': 'T'}, {'pos': 906669, 'rSV': '-'}]
        ref_windows = rSV_window_meta(reference_seq, start_pos)
        #print(f"ref_windows: {ref_windows}")
        number = 1
        for seq_id, sequence in sequences:
            if seq_id == reference_id:
                continue  
                
            var_windows = rSV_window_meta(sequence, start_pos)
            #print(f'var_windows: {type(var_windows)}')
            number += 1
            # åœ¨è¿™é‡Œè¿›è¡Œé€ä¸ªæ¯”å¯¹
            diff_array, meta_array = compare_rSVs_with_meta(ref_windows, var_windows)                
            results.append({
                'chromosome_group': group_name,
                'sequence_id': seq_id,
                'diff_array': diff_array,
                'meta_array': meta_array
            })

        return {
            'file_name': file_name,
            'results': results,
            'error': None
        }
        
    except Exception as e:
        logger.error(f"Error processing sequences in {file_name}: {str(e)}")
        return {'file_name': file_name, 'results': [], 'error': str(e)}




def process_fasta_files(
    directory: str,
    has_insertion_dict: dict, 
    genome_metadata: dict,  # Add genome_metadata parameter
    max_workers: Optional[int] = None,
    output_file: str = None,
    error_log: str = None
) -> Dict:
    """
    Process FASTA files and generate comparison results, integrating genome metadata.
    """
    try:
        if max_workers is None:
            max_workers = 10
        
        logger.info(f"Starting FASTA processing with {max_workers} workers")
        fasta_contents = read_fasta_files(directory)
        results = []
        errors = []
        
        with ProcessPoolExecutor(max_workers=max_workers) as executor:
            futures = {
                executor.submit(process_sequences, file_name, sequences, genome_metadata, has_insertion_dict): file_name
                for file_name, sequences in fasta_contents.items()
            }
            
            for future in tqdm(as_completed(futures), total=len(futures), desc="Generating rSVs"\
                               , unit='group'
                               ):
                result = future.result()
                if result['error']:
                    errors.append(f"Error in {result['file_name']}: {result['error']}")
                if result['results']:
                    results.extend(result['results'])
        
        if output_file:
            df = pd.DataFrame(results)
            df.to_csv(output_file, index=False)
            logger.info(f"Initial results saved to: {output_file}")
        
        if error_log and errors:
            with open(error_log, 'w') as f:
                for error in errors:
                    f.write(f"{error}\n")
            logger.warning(f"Errors were logged to: {error_log}")
        return {'processed': results, 'errors': errors}
    
    except Exception as e:
        logger.error(f"Error in process_fasta_files: {str(e)}")
        raise

def retain_changed_columns_group(rows: List[List[int]]) -> List[List[int]]:
    """
    rows: List of lists[int], each row is a diff_array: [0,1,0,1...].
    The function checks for changes across all rows in each column
    and returns a 2D list with only the columns where changes occurred.
    """
    if not rows:
        return []

    num_rows = len(rows)
    num_cols = len(rows[0])

    # Initialize the result list, keep the first column as is
    retained = [[row[0]] for row in rows]

    # Compare each column starting from the second one
    for col in range(1, num_cols):
        retain = False
        # Check each row to see if the current column differs from the previous column
        for row in range(num_rows):
            if row[col] != row[col - 1]:
                retain = True
                break

        # If there is a change, retain this column in all rows
        if retain:
            for row_i in range(num_rows):
                retained[row_i].append(rows[row_i][col])

    return retained

### <-- NEW OR MODIFIED CODE ###
def retain_changed_columns_group_with_index(rows: List[List[int]]) -> Tuple[List[List[int]], List[int]]:
    """
    Retain columns with changes and return their indices in the original rows[?].
    """
    if not rows:
        return [], []
    num_rows = len(rows)
    num_cols = len(rows[0])

    retained = [[] for _ in range(num_rows)]
    retained_indices = []

    # Retain the first column directly
    for r in range(num_rows):
        retained[r].append(rows[r][0])
    retained_indices.append(0)

    for col in range(1, num_cols):
        keep = False
        for row in rows:
            if row[col] != row[col - 1]:
                keep = True
                break
        if keep:
            for r in range(num_rows):
                retained[r].append(rows[r][col])
            retained_indices.append(col)

    return retained, retained_indices



def process_group(chrom, group, has_insertion_dict, cutoff):
    """
    å¤„ç†æ¯ä¸ªchromçš„ç»„ï¼Œè¿”å›å¤„ç†ç»“æœã€‚
    """
    '''
    # chrom:Group_2_1_1210539
    group:    
    chromosome_group        sequence_id    diff_array        meta_array
    0  Group_2_1_1210539        seq1  [0, 1, 1, 1, 1]  [{'pos': 1210539, 'ref': 'G', 'alt': 'G'}, {'p...
    1  Group_2_1_1210539        seq2  [0, 1, 1, 1, 1]  [{'pos': 1210539, 'ref': 'G', 'alt': 'G'}, {'p...
    2  Group_2_1_1210539        seq3  [0, 1, 1, 1, 1]  [{'pos': 1210539, 'ref': 'G', 'alt': 'G'}, {'p...
    '''
    diff_arrays = []
    meta_arrays = []
    sequence_ids = []
    
    for _, row in group.iterrows():
        try:
            diff_array = eval(row['diff_array']) if isinstance(row['diff_array'], str) else row['diff_array']
            meta_array = eval(row['meta_array']) if isinstance(row['meta_array'], str) else row['meta_array']
            
            if not diff_array or not meta_array:
                logger.warning(f"Skipping empty arrays: chromosome_group={chrom}, sequence_id={row['sequence_id']}")
                continue
                
            diff_arrays.append(diff_array)
            meta_arrays.append(meta_array)
            sequence_ids.append(row['sequence_id'])
        except Exception as e:
            logger.warning(f"Error processing row data: {str(e)}, skipping this row. chromosome_group={chrom}, sequence_id={row['sequence_id']}")
            continue
    
    if len(diff_arrays) < 1 or len(meta_arrays) < 1:
        logger.warning(f"Group {chrom} does not have enough valid data for processing")
        return None
    
    # ä½¿ç”¨ retain_changed_columns_group_with_meta å¤„ç†æ•°æ®
    retained_diff, retained_meta, nrSV_set = retain_changed_columns_group_with_meta(diff_arrays, meta_arrays, chrom, has_insertion_dict, cutoff)
    if not retained_diff or not retained_meta or len(retained_diff) != len(sequence_ids):
        logger.warning(f"Invalid processing results for group {chrom}")
        return None
    
    #print(f'nrSV_set:{nrSV_set}')
    
    if nrSV_set == set():
        nrSV_merged_results = []
        nrSV_dict = {}
    else:
        nrSV_merged_results, nrSV_dict = nrSV_convert(chrom, nrSV_set)
    # å¤„ç†ç»“æœ
    merged_results = []
    for i, seq_id in enumerate(sequence_ids):
        if i < len(retained_diff) and i < len(retained_meta):
            processed_meta = []
            for meta in retained_meta[i]:
                processed_meta.append({
                    'pos': meta['pos'],
                    'ref': meta['ref'],  
                    'alt': meta['alt']   
                })
            
            merged_results.append({
                'chromosome_group': chrom,
                'sequence_id': seq_id,
                'diff_array': retained_diff[i],
                'meta_array': processed_meta
            })
    #print('nrSV_merged_results: ', nrSV_merged_results)
    #print(f'merged_results: {merged_results}')
    return merged_results, nrSV_merged_results, nrSV_dict

import ast
from collections import defaultdict
from typing import List

def nrSV_convert(chrom: str, nrSV_set: set) -> List[dict]:
    merged_results = []
    nrSV_step = 0 # å‚¨å­˜ä¸€ä¸ªæ­¥è¿›å˜é‡
    nrSV_dict = {'group':chrom, 'SV':[]} # å‚¨å­˜ä¸€ä¸ªdictç”¨äºåç»­vcfæ–‡ä»¶çš„sampleä¾¿æ·æŸ¥æ‰¾å¡«è¡¥
    # å°†å­—ç¬¦ä¸²è½¬æ¢ä¸ºå­—å…¸
    sv_records = [ast.literal_eval(item) for item in nrSV_set]
    #print(f'sv_records{sv_records}')
    #[{'pos': 1210539, 'ref': 'G', 'alt': 'G', 'SV': 1}, {'pos': 1210542, 'ref': '-', 'alt': 'G', 'SV': 1}, {'pos': 1210541, 'ref': '-', 'alt': 'G', 'SV': 1}, {'pos': 1210543, 'ref': '-', 'alt': 'G', 'SV': 1}]
    # æ ¹æ® SV å€¼è¿›è¡Œåˆ†ç»„
    groups = defaultdict(list)
    for rec in sv_records:
        groups[rec['SV']].append(rec)
    
    # å¯¹æ¯ä¸ª SV ç»„ï¼ŒæŒ‰ pos æ’åºåç”Ÿæˆ meta_arrayï¼Œå¹¶ç”Ÿæˆä¸€è¡Œç»“æœ
    for sv_value, records in sorted(groups.items()):
        #print(f'sv_value: {sv_value}')
        nrSV_dict['SV'].append(sv_value)
        # æŒ‰ pos æ’åº
        records.sort(key=lambda x: x['pos'])
        pos = records[0]['pos']
        ref = records[0]['ref']
        #print(f'sv_value:{sv_value}, records:{records}')
        #sv_value:1, records:[{'pos': 1210539, 'ref': 'G', 'alt': 'G', 'SV': 1}, {'pos': 1210541, 'ref': '-', 'alt': 'G', 'SV': 1}, {'pos': 1210542, 'ref': '-', 'alt': 'G', 'SV': 1}, {'pos': 1210543, 'ref': '-', 'alt': 'G', 'SV': 1}]
        merged = []  # å­˜æ”¾åˆå¹¶åçš„ç»“æœ
        current = records[0].copy()   # ä»¥ç¬¬äºŒä¸ªå…ƒç´ ä½œä¸ºåˆå§‹
        #print(f'records:{records}')
        if len(records) == 1:
            merged = records
        else:
            for rec in records[1:]:  # ä»ç¬¬2ä¸ªå¼€å§‹éå†
                next_pos = current["pos"] + len(current["ref"])  # è®¡ç®—åˆå¹¶åä¸‹ä¸€ä¸ªæœŸæœ›ä½ç½®
                
                if rec["pos"] == next_pos: 
                    # åˆå¹¶æ’å…¥çªå˜
                    current["ref"] += rec["ref"]  # ref ç»§ç»­æ‰©å±•
                    current["alt"] += rec["alt"]  # alt ä¹Ÿç´¯åŠ 
                else:
                    # ä¸èƒ½åˆå¹¶ï¼Œå­˜å‚¨å½“å‰åˆå¹¶å—ï¼Œå¹¶é‡æ–°å¼€å§‹
                    merged.append(current)
                    current = rec.copy()

            # åˆ«å¿˜äº†æœ€åä¸€ä¸ªç‰‡æ®µ
            merged.append(current)
        #print(f'merged:{merged}')
        # åˆå¹¶æ‰€æœ‰è®°å½•çš„ meta ä¿¡æ¯
        meta_array = [{'pos': pos, 'ref': ref+rec['ref'].replace('-',''), 'alt': ref+rec['alt'].replace('-','')} for rec in merged[1:]]
        nrSV_count = len(meta_array)
        for i in range(nrSV_count):
            merged_results.append({
                'chromosome_group': chrom + f'_nrSV{i+1+nrSV_step}',
                'sequence_id': f"seq{sv_value}",  # SV å€¼å¯¹åº”åºåˆ—å·ï¼Œå¦‚ SV==1 -> seq1
                'meta_array': meta_array[i]
            })
        nrSV_step += nrSV_count
    
    # å¦‚æœæœ‰å¤šä¸ªç»„ï¼ŒæŒ‰åºå·æ’åºï¼ˆè¿™é‡Œ sequence_id æ ¼å¼ä¸º "seq{number}"ï¼‰
    merged_results.sort(key=lambda x: int(x['chromosome_group'].split('_')[4][4:]))
    # print(f'merged_results: {merged_results}')
    # [{group:, seqID:, meta_array:[{},{},...,{}]}]
    return merged_results, nrSV_dict

import pysam
def nrSV_vcf_generate(nrSV_csv: str, config, nrSV_list, nSV_name: str):
    df = pd.read_csv(nrSV_csv) 
    nrSV_count = df.shape[0]
    output_cache = {}  # å­˜å‚¨ç»“æœçš„å­—å…¸
    # æ‹¿åˆ°outputçš„vcfçš„åå­—
    output_vcf = os.path.join(config.output_dir, nSV_name)
    # è¯»å–åŸæœ¬vcfçš„ä¿¡æ¯
    with pysam.VariantFile(config.vcf_file, "r") as vcf_in:
        for item in nrSV_list:
            parts = item['group'].split('_')
    
            chrom = parts[1]
            pos = int(parts[3])
            sv_indexes = item['SV']
            # åˆå§‹åŒ–å­˜å‚¨è¯¥ group çš„åŸºå› å‹æ•°æ®
            output_cache[item['group']] = {}
        
            # éå† VCF è¯¥ä½ç‚¹çš„æ‰€æœ‰å˜å¼‚
            sv_records = []
            for record in vcf_in.fetch(chrom, pos - 1, pos):  # pos-1 å› ä¸º VCF ä½¿ç”¨ 1-based ä½† fetch ä½¿ç”¨ 0-based
                if record.pos == pos:
                    sv_records.append(record)
        
            # æå– SV ç´¢å¼•å¯¹åº”çš„å˜å¼‚
            for sv_idx in sv_indexes:
                if sv_idx <= len(sv_records):  # ç¡®ä¿ç´¢å¼•æœ‰æ•ˆ
                    sv_record = sv_records[sv_idx - 1]  # SV ç´¢å¼•æ˜¯ä» 1 å¼€å§‹çš„ï¼Œè€Œ Python list æ˜¯ 0-based

                    # è·å–æ‰€æœ‰ sample çš„ GT ä¿¡æ¯
                    gt_data = {sample: sv_record.samples[sample]['GT'] for sample in sv_record.samples}

                    # å­˜å‚¨åŸºå› å‹æ•°æ®
                    output_cache[item['group']][f"SV{sv_idx}"] = gt_data 
                    # print(f'idx:{sv_idx},gt:{gt_data}')
    # è·å– output_cache é‡Œç¬¬ä¸€ä¸ªå¯ç”¨çš„ group_name å’Œ SV_x ä½œä¸º sample_names çš„æ¥æº
    sample_names = []

    if output_cache:  # ç¡®ä¿ç¼“å­˜ä¸ä¸ºç©º
        first_group = next(iter(output_cache))  # è·å–ç¬¬ä¸€ä¸ª group_name
        first_sv = next(iter(output_cache[first_group])) if output_cache[first_group] else None  # è·å–ç¬¬ä¸€ä¸ª SV_x

        if first_sv:
            sample_names = list(output_cache[first_group][first_sv].keys())  # è·å– sample åç§°åˆ—è¡¨
    # å†™å…¥è¾“å‡º
    with open(output_vcf, "a") as f_out:
        for _, row in df.iterrows():
            group_name_parts = row['chromosome_group'].split('_')
            group_name = '_'.join(group_name_parts[:-1])
            nrSV_id = row['chromosome_group']
            chrom = group_name_parts[1]
            meta = ast.literal_eval(row['meta_array'])
            sv_id = int(row['sequence_id'][3:])
            ref = meta['ref']
            alt = meta['alt']
            pos = meta['pos']
            try:
                gt_info = output_cache[group_name][f'SV{sv_id}']
            except KeyError as e:
                print(f"KeyError: {e} not found in output_cache")
                gt_info = {}
            gt_values = ["/".join(map(str, gt_info.get(sample, ('.', '.')))) for sample in sample_names]
            f_out.write(f"{chrom}\t{pos}\t{nrSV_id}\t{ref}\t{alt}\t.\t.\ttype=nrSV\tGT\t" + "\t".join(gt_values) + "\n")
    return nrSV_count

def process_and_merge_results(results, output_csv: str, threads: str, \
                              has_insertion_dict: dict, cutoff: float, nrSV_csv: str):
    """
    Process and merge k-mer window results, removing '-' characters in the final results.
    """
    # has_insertion_dict:{'Group_2_1_1210539': True}
    try:
        df = pd.DataFrame(results['processed'])
    except Exception as e:
        logger.error(f"Error saving results to CSV: {str(e)}")
        raise
        
    try:
        if df.empty:
            logger.warning("Input file is empty")
            return
        
        # æ ¹æ®chromosome_groupåˆ†ç»„
        grouped = df.groupby('chromosome_group')
        
        # è®¡ç®—è¿›åº¦æ¡æ€»ä»»åŠ¡æ•°ï¼šæ‰€æœ‰ç»„çš„æ•°é‡
        total_groups = len(grouped)
        
        merged_results = []
        nrSV_merged_results = []
        nrSV_list = []
        # ä½¿ç”¨è¿›åº¦æ¡
        with tqdm(total=total_groups, desc="c. Merging rSVs", unit="group") as pbar:
            # ä½¿ç”¨ProcessPoolExecutoræ¥å¹¶è¡Œå¤„ç†æ¯ä¸ªç»„
            with ProcessPoolExecutor(max_workers=threads) as executor:
                future_to_group = {executor.submit(process_group, chrom, group, has_insertion_dict, cutoff): chrom for chrom, group in grouped}
                # éå†æ¯ä¸ªä»»åŠ¡çš„ç»“æœ
            for future in as_completed(future_to_group):
                try:
                    result, nrSV_result, nrSV_dict = future.result()
                    if result:
                        merged_results.extend(result)
                    if nrSV_result:
                        nrSV_merged_results.extend(nrSV_result)
                    if nrSV_dict:
                        nrSV_list.append(nrSV_dict)
                except Exception as e:
                    group = future_to_group[future]
                    print(f"âŒ Error in processing group {group}: {e}")
                    print(f"ğŸ’¡ Tip: Please check whether the sequences in group {group} are normalized properly.")
                pbar.update(1)

        if not merged_results:
            logger.warning("No valid merged results")
            return
        
        # åˆ›å»ºæ–°çš„DataFrameå¹¶ä¿å­˜
        merged_df = pd.DataFrame(merged_results)
        merged_df.to_csv(output_csv, index=False)
        logger.info(f"Merged results of rSV saved to: {output_csv}")
        # ç”ŸæˆnrSV_results.csv
        nrSV_merged_df = pd.DataFrame(nrSV_merged_results)
        nrSV_merged_df.to_csv(nrSV_csv, index=False)
        logger.info(f"Merged results of nrSV saved to: {nrSV_csv}")
    except Exception as e:
        logger.error(f"Error during processing: {str(e)}")
        raise
    return nrSV_list
    # print(f'mergeè¾“å…¥threads:{threads}')

def process_chromosome_groups(input_csv: str, output_csv: str) -> None:
    """
    Process data for each chromosome_group, retaining arrays for all positions.
    """
    data = defaultdict(list)
    
    # Read CSV file
    with open(input_csv, newline='', encoding='utf-8') as csvfile:
        reader = pd.read_csv(csvfile)
        
        for _, row in reader.iterrows():
            chromosome_group = row['chromosome_group']
            diff_array = eval(row['diff_array']) if isinstance(row['diff_array'], str) else row['diff_array']
            meta_array = eval(row['meta_array']) if isinstance(row['meta_array'], str) else row['meta_array']
            
            # Directly add original data to processed_data
            #processed_data.append({
            #    'chromosome_group': chromosome_group,
            #    'sequence_id': row['sequence_id'],
            #    'diff_array': diff_array,  # Use original diff_array
            #    'meta_array': meta_array   # Use original meta_array
            #})
    
    # Write processed CSV
    #processed_df = pd.DataFrame(processed_data)
    # Convert arrays to strings for storage
    #processed_df['diff_array'] = processed_df['diff_array'].apply(str)
    #processed_df['meta_array'] = processed_df['meta_array'].apply(str)
    #processed_df.to_csv(output_csv, index=False, encoding='utf-8')
    logger.info(f"Processed results saved to: {output_csv}")


def save_rSV_results_to_csv(results: Dict, output_file: str) -> None:
    """Save rSV comparison results to a CSV file."""
    try:
        if results['processed']:
            df = pd.DataFrame(results['processed'])
            df.to_csv(output_file, index=False)
            logger.info(f"Results saved to: {output_file}")
        else:
            logger.warning("No results to save")
    except Exception as e:
        logger.error(f"Error saving results to CSV: {str(e)}")
        raise


### <-- NEW OR MODIFIED CODE ###
def explode_final_results(input_csv: str, output_csv: str) -> None:
    """
    To expand meta_array into multiple rows, one k-mer per row, and write pos/ref/alt/diff information
    to the table, use this function as an example.
    """
    df = pd.read_csv(input_csv)
    # Assuming there are 'chromosome_group', 'sequence_id', 'diff_array', 'meta_array'
    all_rows = []
    for _, row in df.iterrows():
        cg = row['chromosome_group']
        seq_id = row['sequence_id']
        diff_array = eval(row['diff_array']) if isinstance(row['diff_array'], str) else row['diff_array']
        meta_array = eval(row['meta_array']) if isinstance(row['meta_array'], str) else row['meta_array']

        for diff_val, meta in zip(diff_array, meta_array):
            # meta: {'pos':..., 'ref':..., 'alt':...}
            out_row = {
                'chromosome_group': cg,
                'sequence_id': seq_id,
                'pos': meta['pos'],
                'ref': meta['ref'],
                'alt': meta['alt'],
                'difference': diff_val
            }
            all_rows.append(out_row)

    df_out = pd.DataFrame(all_rows)
    df_out.to_csv(output_csv, index=False)
    logger.info(f"Exploded results saved to: {output_csv}")

def retain_changed_columns_group_with_meta(
    rows: List[List[int]], 
    meta_rows: List[List[Dict]],
    chrom,
    has_insertion_dict,
    cutoff # æ³¨æ„æ˜¯ä¸ªfloatï¼Œæ˜¯ç™¾åˆ†æ¯”
) -> Tuple[List[List[int]], List[List[Dict]]]: # åˆå¹¶å‡½æ•°
    """
    Process difference arrays and metadata arrays, merging identical columns starting from the first column.
    
    Args:
        rows: List of difference arrays, each row represents the alignment result of a sequence
        meta_rows: List of metadata, corresponding to the difference arrays
    
    Returns:
        Tuple[List[List[int]], List[List[Dict]]]: (Processed difference arrays, Processed metadata arrays)
    """
    if not rows or not meta_rows:
        return [], []
    
    if len(rows) != len(meta_rows):
        raise ValueError(f"Mismatch between the lengths of difference arrays and metadata arrays, meta_data_pos:{meta_rows[0][0]['pos']}")
    # è°ƒè¯•print meta_rows
    # print(f"Mismatch between the lengths of difference arrays and metadata arrays, meta_data_pos:{meta_rows[0][0]['pos']}")
    # print(f'æ­£åœ¨å¤„ç†:{chrom}')
    num_rows = len(rows)
    num_cols = len(rows[0])
    #print(f'num_rows:{num_rows}, num_cols:{num_cols}')

    # Initialize result lists, è¿™ä¸ªåˆå§‹åŒ–çœ‹ä¸Šå»æ˜¯æŠŠç¬¬ä¸€ä¸ªä¸œè¥¿å…ˆæ‰”ä¸Šå»
    retained_diff = [[rows[i][0]] for i in range(num_rows)]
    retained_meta = [[meta_rows[i][0]] for i in range(num_rows)]
    #print(f'diff:{rows}, meta:{meta_rows}\nretained diff:{retained_diff}, meta: {retained_meta}')
    # diff:[[0, 1, 1, 1, 1, 1, 1], [1, 1, 0, 0, 0, 0, 0]]
    # retained diff:[[0], [1]]ï¼Œä¿ç•™äº†æ¯ç»„ç¬¬ä¸€ä¸ª
    current_col = 0  # Current column index being processed
    
    # Compare starting from the second column
    for col in range(1, num_cols):
        is_same = True
        # Check if all rows in the current column are the same as the previous column
        for row_i in range(num_rows):
            if rows[row_i][col] != rows[row_i][col-1]:
                is_same = False
                break
        
        if is_same:
            # If the current column is the same as the previous column, merge metadata
            for row_i in range(num_rows):
                curr_meta = meta_rows[row_i][col]
                prev_meta = retained_meta[row_i][-1]
                
                # Merge metadata, adding new bases
                merged_meta = {
                    'pos': prev_meta['pos'],
                    'ref': prev_meta['ref'] + curr_meta['ref'][-1],
                    'alt': prev_meta['alt'] + curr_meta['alt'][-1]
                }
                retained_meta[row_i][-1] = merged_meta
        else:
            # If different, add new column
            for row_i in range(num_rows):
                retained_diff[row_i].append(rows[row_i][col])
                retained_meta[row_i].append(meta_rows[row_i][col])
            current_col += 1

    nrSV_set = set()
    new_rSV_count = 0 # è®°å½•å¤šå°‘æ–°çš„rSVç”±äºé‡å ä½†ä¸ç›¸åŒè€Œæ‹†å¼€
    ## ä¸‹é¢å¼€å§‹å†™å¯¹poly_insä¸­å¯èƒ½çš„poly_altçš„æƒ…å†µçš„diffå’Œmetaçš„æ”¹åŠ¨ä»£ç 
    # é¦–å…ˆåˆ¤æ–­ä¸€ä¸‹æ˜¯ä¸æ˜¯poly_insç»„ï¼Œä¸æ˜¯å°±ç›´æ¥è·³è¿‡äº†
    if has_insertion_dict[chrom]:# ä¹Ÿå°±æ˜¯if trueï¼Œ{'Group_2_1_1210539': True} 
        sv_count = len(retained_diff) # å…ˆç¡®å®šå¾ªç¯çš„æ¬¡æ•°ï¼Œä¹Ÿå°±æ˜¯svçš„ä¸ªæ•°
        rSV_count = len(retained_diff[0]) # ç¡®å®šåˆå§‹rSVçš„ä¸ªæ•°
        rSV_index_small = 0 # åç§»å€¼
        poly_pos = 0 # ç”¨äºå‚¨å­˜posçš„indexçš„æ­¥è¿›è¡¥å¿ï¼ˆç”¨äºåç»­çš„æ ‡å‡†åŒ–ï¼‰

        for i in range(1, rSV_count): # éå†ç°åœ¨çš„rSVï¼ˆç¬¬ä¸€ä¸ªå‡‘æ•°çš„ä¸ç”¨çœ‹æ‰€ä»¥ä»1å¼€å§‹ï¼‰

            rSV_index = i + rSV_index_small #####ç”¨äºè¡¥å¿ç”±äºæ’å…¥å¸¦æ¥çš„iåç§»
           
            #print(f'i:{i},rSV_index:{rSV_index}') 
            alt_list = [] # åˆ›å»ºä¸€ä¸ªåˆ—è¡¨ï¼Œç”¨äºæ— å·®åˆ«å­˜å‚¨altä»¥è·å–å¤šaltçš„é‡å¤æƒ…å†µ
            alt_set = set() # åˆ›å»ºä¸€ä¸ªé›†åˆï¼Œæ¥çœ‹æ˜¯å¦æ˜¯å¤šalt
           
            for j in range(sv_count): # å¯¹å›ºå®šrSVï¼ˆåˆ—ï¼‰çš„æ¯è¡Œè¿›è¡Œéå†
                if retained_diff[j][rSV_index] == 1 \
                    and retained_meta[j][rSV_index]['alt'][0] != "-" : # å¦‚æœå¯¹åº”çš„ç¬¬iä¸ªrSVçš„SVï¼ˆéå†jï¼‰ä¸º1ï¼Œä¸”altä¸ä¸ºç©ºï¼Œåˆ™æ·»åŠ åˆ°é›†åˆé‡Œ
                    #start_count = len(alt_set) 
                    alt_origin = retained_meta[j][rSV_index]['alt']
                    alt_set.add(alt_origin) #  'alt': 'GGGG'
                    alt_list.append(alt_origin) # å­˜å‚¨ä¸€ä¸ªé›†åˆå’Œä¸€ä¸ªåˆ—è¡¨
                    #end_count = len(alt_set)
                    #if start_count != end_count:
                        #alt_index_list.append(j) # è·å–å¯¹åº”çš„altçš„ç´¢å¼•
            
            #print(f'alt_index_list:{alt_index_list}')
            #print(f'alt_set:{alt_set}')
            poly_alt_count = len(alt_set) # æŸ¥ä¸€ä¸‹æœ‰å¤šå°‘ä¸ªalt
                
            if poly_alt_count <= 1: # é‚£å°±è¯´æ˜æ²¡æœ‰å¤šaltçš„æƒ…å†µäº†ï¼Œåªéœ€è¦å¤„ç†pos
                for j in range(sv_count): # å¯¹å¯¹åº”rSVçš„æ‰€æœ‰SVçš„posè¿›è¡Œå¤„ç†
                    retained_meta[j][rSV_index]['pos'] += poly_pos
            # åœ¨è¿™é‡Œè¯•ç€æ·»åŠ altç›¸ä¼¼é˜ˆå€¼ï¼Œé€»è¾‘ï¼šä¿©ä¿©æ¯”è¾ƒï¼Œåªè¦æœ‰ä¸€ä¸ªæ»¡è¶³é˜ˆå€¼åˆ™ä¿ç•™ï¼Œä¸€ä¸ªéƒ½ä¸æ»¡è¶³åˆ™å‰”é™¤
            

            else:  # ä¸ç„¶éœ€è¦å¯¹diff_arrayå’Œmeta_arrayè¿›è¡Œå¡«å……å¤„ç†
                '''é¦–å…ˆå¯¹é‡å çš„altè¿›è¡Œæå–ï¼Œä¹Ÿå°±æ˜¯alt_set:{'G', 'A'}ç±»çš„é€šè¿‡filterä¹‹åæˆ‘ä»¬ä¸æŠŠè¿™ä¸ªå½“æˆæ–°çš„rSV'''
                ## ä¸‹é¢æ·»åŠ é˜ˆå€¼
                alt_set_filtered = filter_similar_sequences(alt_list, cutoff) # è¾“å…¥ä¸€ä¸ªé˜ˆå€¼
                #print(f'alt_list:{alt_list} alt_set_filtered:{alt_set_filtered}')
                ###########################################
                # step 0. å¦‚æœä¸€ä¸ªæ²¡ä¿ç•™ï¼Œç›´æ¥æŠŠåˆ—åˆ äº†ã€‚
                ###########################################
                if alt_set_filtered == set(): # è¯´æ˜ä¸€ä¸ªéƒ½æ²¡ä¿ç•™, éœ€è¦åˆ é™¤è¯¥rSVçš„diff_arrayå’Œmeta_array
                    
                    poly_pos -= len(retained_meta[0][rSV_index]['ref'])
                    for j in range(sv_count):
                        retained_diff[j].pop(rSV_index) # åˆ é™¤diff
                        nrSV_meta = retained_meta[j][rSV_index] # {ref:xxx, alt:xxx, pos:xxx}
                        nrSV_meta['SV'] = j+1 # {ref:xxx, alt:xxx, pos:xxx, SV:xxx}
                        nrSV_set.add(str(nrSV_meta)) # "{ref:xxx, alt:xxx, pos:xxx, SV:xxx}"
                        meta0 = retained_meta[j][0] # {ref: A, alt : A}, æœ€å‰é¢ç”¨äºå®šä½çš„
                        meta0['SV'] = j+1
                        nrSV_set.add(str(meta0))


                        retained_meta[j].pop(rSV_index) # åˆ é™¤meta
                    # å»æ­¥è¿›
                    rSV_index_small -= 1 

                else:
                    
                    new_rSV_count += len(alt_set_filtered) - 1
                    #print(f'alt_set_filtered:{alt_set_filtered}')
                    # å¾—åˆ°çš„setï¼šä» ["AAAAAAA", "AAAAAAA", "AAAAAAG", "GGGGGGG", "GGCACAG"]
                    # é˜ˆå€¼ä¸º90%å¾—åˆ° {'AAAAAAA'}
                    #############################################
                    # step 1: å…ˆæ‹¿åˆ°äº†ä¿ç•™é‡å é¡¹çš„alt_set_filtered
                    #############################################
                    #print('å¼€å§‹å¤„ç†å¤šalt')
                    poly_alt = 0 # ç”¨äºå‚¨å­˜altçš„indexçš„æ­¥è¿›
                    alt_index_list = [] # åˆ›å»ºä¸€ä¸ªåˆ—è¡¨ï¼Œå¾—åˆ°åç»­å“ªä¸ªSVæ˜¯poly_altçš„ï¼Œæ¯”å¦‚[2,3]å¯¹åº”äº†alt_setçš„çš„{'AAA','CCC'}
                    meta_list = [] # å‚¨å­˜ä¸€ä¸ªä¸è¢«indexæ±¡æŸ“çš„metalist
                    alt_nrSV_index_set = set() # {1,2...} nrSVçš„å¯¹åº”çš„j
                    #############################################
                    # step 2: å¤„ç†éç©ºæœªé‡å é¡¹ï¼ˆnrSVï¼‰
                    #############################################
                    for j in range(sv_count): ###å…ˆéå†SVå¾—åˆ°è¢«æ’é™¤çš„nrSVçš„meta
                        alt = retained_meta[j][rSV_index]['alt'] # æ¯”å¦‚è¯´GGG
                        meta = retained_meta[j][rSV_index] # ç¼“å­˜ä¸€ä¸‹meta                   
                        if alt not in alt_set_filtered and alt[0] != "-":# å¦‚æœaltå¯¹ä¸ä¸Šï¼Œå¹¶ä¸”éç©ºï¼Œé‚£ä¹ˆè¯´æ˜æ²¡æœ‰é‡å¤ä¹Ÿæ²¡è¾¾åˆ°é‡å çš„é˜ˆå€¼
                            meta['SV'] = j+1 # ç»™metaå¯¹åº”çš„å­—å…¸å¤šåŠ ä¸€ä¸ªSVçš„å¯¹åº”ç¼–å·ï¼ˆæˆ–è®¸ä¸éœ€è¦+1ï¼Œå†çœ‹ï¼‰
                            # ç°åœ¨metaæ˜¯{'pos':xxxx, 'ref':xxxxx, 'alt':xxxxx, 'SV':x}
                            nrSV_set.add(str(meta)) # åµŒå¥—ï¼Œä½†æ˜¯å¤©ç„¶å»é‡
                            meta0 = retained_meta[j][0] # {ref: A, alt : A}, æœ€å‰é¢ç”¨äºå®šä½çš„
                            meta0['SV'] = j+1  
                            nrSV_set.add(str(meta0))                          
                            # setï¼š{{ref:xxx, alt:xxx .....}}
                            # æ¥ç€æŠŠåŸå§‹ä½ç½®çš„diffææˆ0, è¡¨ç¤ºè¿™ä¸ªæ— å…³äº†
                            retained_diff[j][rSV_index] = 0
                            alt_nrSV_index_set.add(j)
                            #print(f'meta_nrSV_set:{meta_nrSV_set}')
                        #nrSV_meta_list_all.append(meta_nrSV_set)

                    for alt_filtered in alt_set_filtered:  # éå†æ¯ä¸ªå€™é€‰altï¼Œæ¯”å¦‚è¯´ AAA
                        found = False  # ç”¨äºæ ‡è®°æ˜¯å¦æ‰¾åˆ°å¯¹åº”çš„alt
                        for j in range(sv_count):  # éå†æ‰€æœ‰ SV
                            #ä¸‹é¢è¯•ç€æ·»åŠ ä¸€ä¸‹ï¼ŒnrSVçš„å†…å®¹
                            if retained_meta[j][rSV_index]['alt'] == alt_filtered and not found:  # ç¬¬ä¸€æ¬¡æ‰¾åˆ°å¯¹åº” alt
                                alt_index_list.append(j)  # è®°å½• sv ä½ç½®
                                found = True  # æ ‡è®°æ‰¾åˆ°ï¼Œä¿è¯åªä¼šæ‰¾åˆ°çš„ç¬¬ä¸€æ¬¡æ¥åˆ°è¿™ä¸ªä½ç½®
                                meta = retained_meta[j][rSV_index] # ç¼“å­˜ä¸€ä¸‹meta
                                break

                        meta['pos'] += poly_pos
                        #last_length = len(meta['alt']) # å‚¨å­˜ä¸€ä¸ªlast_lengthç”¨äºæ­¥è¿›å‰Šå‡
                        #poly_pos += last_length # å‚¨å­˜æ­¥è¿›
                        #print(f"meta:{meta},len:{len(meta['alt'])}")
                        meta_list.append(meta)        
                    #print(f'meta_list:{meta_list},alt_index_list:{alt_index_list}')
                    _count_dict = {} # åˆå§‹åŒ–å ä½ç¬¦countçš„å­—å…¸ç”¨äºæŸ¥éªŒ
                    
                    for index, alt_index in enumerate(alt_index_list): # éå†altçš„indexï¼Œæ¯”å¦‚å¯¹ç¬¬ä¸€ä¸ªSVè¿›è¡ŒrSVçš„æ‰©å¼ å˜æ¢
                        meta = meta_list[index] # è¿™é‡Œåº”è¯¥æ˜¯ä¸€ä¸ªmeta={}ï¼Œè¡¨ç¤ºå¯¹åº”çš„altçš„meta
                        alt_to_compare = retained_meta[alt_index][rSV_index+ poly_alt]['alt'] # åˆ—è¡¨ä¸­è¿›è¡Œæ¯”è¾ƒçš„
                        #print(f'å¯¹ç¬¬alt_index={alt_index}çš„å¾ªç¯ï¼Œmetaä¸º{meta}')                 
                        for j in range(sv_count): # å¯¹æŸä¸€ä¸ªå…·ä½“çš„å˜åŒ–è¿›è¡Œé€ä¸ªéå†
                            # è®°å½•altå‡ºç°å ä½ç¬¦çš„æ¬¡æ•°
                            #print(f'there, j={j}, index={rSV_index+poly_alt},retained_meta[j]={retained_meta[j]},')
                            if rSV_index + poly_alt < len(retained_meta[j]) :
                                alt = retained_meta[j][rSV_index+ poly_alt]['alt'] # 
                            else:
                                alt = None
                            #print(f'here, j={j}, alt=:{alt}')
                            alt_del = retained_meta[j][rSV_index+ poly_alt-index]['alt'] #å¯¹delè¿›è¡Œå»æ­¥è¿›                             
                            #print(f"{alt}å¯¹æ¯”{alt_to_compare}")
                            if alt_del[0] == "-" or j in alt_nrSV_index_set: # å¦‚æœæ˜¯å ä½ç¬¦æˆ–è€…å› ä¸ºæ— è¶³å¤Ÿé‡å è¦è¢«åˆ æ‰çš„nrSVï¼Œç‰¹æ®Šå¤„ç†
                                if f'SV{j}' not in _count_dict: # å¦‚æœè¯¥SVè¿˜æ²¡å‡ºç°è¿‡
                                    _count_dict[f'SV{j}'] = j
                                    #print(f'1_count_dict:{_count_dict}')
                                    retained_meta[j][rSV_index + poly_alt]['pos'] = meta['pos'] # åªæ”¹pos
                                elif _count_dict[f'SV{j}'] == j: # è¯¥SVå‡ºç°è¿‡ä¸€æ¬¡ï¼Œåˆ™åŠ meta
                                    #print(f'2_count_dict:{_count_dict}')
                                    retained_diff[j][rSV_index + poly_alt] =  0 # å°±å¯¹å¯¹åº”rSVä½ç½®çš„å‰é¢åŠ ä¸€ä¸ª0(insertæ˜¯ç´¢å¼•å‰çš„ä½ç½®æ’å…¥)
                                    retained_meta[j][rSV_index + poly_alt] = meta  
                                
                            elif alt != alt_to_compare: # å¦‚æœå¹¶éè¯¥altå˜å¼‚ï¼Œå°±å¯¹å…¶diffå’Œmetaè¿›è¡Œæ‰©å……

                                #print(f'åˆ°è¿™å„¿3, j={j}')                    
                                retained_diff[j][rSV_index + poly_alt] =  0 # å°±å¯¹å¯¹åº”rSVä½ç½®çš„å‰é¢åŠ ä¸€ä¸ª0(insertæ˜¯ç´¢å¼•å‰çš„ä½ç½®æ’å…¥)
                                retained_meta[j][rSV_index + poly_alt] = meta
                                #print(f'ç¬¬{j}ä¸ªSVï¼Œåœ¨{rSV_index+poly_alt-1}çš„ä½ç½®æ’å…¥äº†{meta}')
                            
                            else:                               
                                retained_meta[j][rSV_index + poly_alt] = meta                                                     
                                #print(f'æˆ‘æ²¡é€šè¿‡,metaæ˜¯{meta},j={j},i={i},rSV_index={rSV_index}')
                        #poly_alt += 1 # å‚¨å­˜æ­¥è¿›
                    
                    #rSV_index_small += len(meta_list)-1
                    #poly_pos -=  last_length# è¿˜å¾—å‡å»polyçš„åç§»å€¼

    return retained_diff, retained_meta, nrSV_set 
# è¾“å‡ºçš„æ˜¯ä¸€ä¸ªç»„çš„æ¯”å¦‚ï¼š
##retained_diff: metaï¼š
# [[0, 1], [[{'pos': 1210539, 'ref': 'G', 'alt': 'G'}, {'pos': 1210540, 'ref': '----', 'alt': 'GGGG'}],
# [0, 1],  [{'pos': 1210539, 'ref': 'G', 'alt': 'G'}, {'pos': 1210540, 'ref': '----', 'alt': 'AAAA'}], 
# [0, 1]], [{'pos': 1210539, 'ref': 'G', 'alt': 'G'}, {'pos': 1210540, 'ref': '----', 'alt': 'ATAA'}]]

##retained_diff: metaï¼š
# [[0, 1],    [[{'pos': 1210539, 'ref': 'G', 'alt': 'G'}, {'pos': 1210540, 'ref': '----', 'alt': 'GGGG'}],
# [0, 0, 1],  [{'pos': 1210539, 'ref': 'G', 'alt': 'G'}, {'pos': 1210540, 'ref': '----', 'alt': 'GGGG'}, {'pos': 1210540, 'ref': '----', 'alt': 'AAAA'}], 
# [0, 0, 1]], [{'pos': 1210539, 'ref': 'G', 'alt': 'G'}, {'pos': 1210540, 'ref': '----', 'alt': 'GGGG'}, {'pos': 1210540, 'ref': '----', 'alt': 'ATAA'}]]

'''
i = 1
diff:
[[0, 0, 1, 1, 1], 
[0, 0, 1, 1, 0], 
[0, 1, 1, 0, 0]]
meta:
[[{'pos': 1210539, 'ref': 'G', 'alt': 'G'}, {'pos': 1210540, 'ref': '-', 'alt': '-'}, {'pos': 1210541, 'ref': '--', 'alt': 'GG'}, {'pos': 1210543, 'ref': '-', 'alt': 'G'}, {'pos': 1210544, 'ref': '-', 'alt': 'G'}], 
[{'pos': 1210539, 'ref': 'G', 'alt': 'G'}, {'pos': 1210540, 'ref': '-', 'alt': '-'}, {'pos': 1210541, 'ref': '--', 'alt': 'AA'}, {'pos': 1210543, 'ref': '-', 'alt': 'A'}, {'pos': 1210544, 'ref': '-', 'alt': '-'}], 
[{'pos': 1210539, 'ref': 'G', 'alt': 'G'}, {'pos': 1210540, 'ref': '-', 'alt': 'T'}, {'pos': 1210541, 'ref': '--', 'alt': 'AA'}, {'pos': 1210543, 'ref': '-', 'alt': '-'}, {'pos': 1210544, 'ref': '-', 'alt': '-'}]]

i = 2
diff:
[[0, 0, 1, 0, 1, 1], 
[0, 0, 0, 1, 1, 0], 
[0, 1, 0, 0, 1, 0, 0]]
meta:
[[{'pos': 1210539, 'ref': 'G', 'alt': 'G'}, {'pos': 1210540, 'ref': '-', 'alt': '-'}, {'pos': 1210541, 'ref': '--', 'alt': 'GG'}, {'pos': 1210543, 'ref': '--', 'alt': 'AA'}, {'pos': 1210543, 'ref': '-', 'alt': 'G'}, {'pos': 1210544, 'ref': '-', 'alt': 'G'}], 
[{'pos': 1210539, 'ref': 'G', 'alt': 'G'}, {'pos': 1210540, 'ref': '-', 'alt': '-'}, {'pos': 1210541, 'ref': '--', 'alt': 'GG'}, {'pos': 1210543, 'ref': '--', 'alt': 'AA'}, {'pos': 1210543, 'ref': '-', 'alt': 'A'}, {'pos': 1210544, 'ref': '-', 'alt': '-'}], 
[{'pos': 1210539, 'ref': 'G', 'alt': 'G'}, {'pos': 1210540, 'ref': '-', 'alt': 'T'}, {'pos': 1210541, 'ref': '--', 'alt': 'GG'}, {'pos': 1210543, 'ref': '--', 'alt': 'AA'}, {'pos': 1210541, 'ref': '--', 'alt': 'AA'}, {'pos': 1210543, 'ref': '-', 'alt': '-'}, {'pos': 1210544, 'ref': '-', 'alt': '-'}]]

i = 2æ”¹
diff:
[[0, 0, 1, 1, 1], 
[0, 0, 0, 1, 1, 0], 
[0, 1, 0, 1, 0, 0]]
mata:
[[{'pos': 1210539, 'ref': 'G', 'alt': 'G'}, {'pos': 1210540, 'ref': '-', 'alt': '-'}, {'pos': 1210541, 'ref': '--', 'alt': 'GG'}, {'pos': 1210543, 'ref': '-', 'alt': 'G'}, {'pos': 1210544, 'ref': '-', 'alt': 'G'}], 
[{'pos': 1210539, 'ref': 'G', 'alt': 'G'}, {'pos': 1210540, 'ref': '-', 'alt': '-'}, {'pos': 1210541, 'ref': '--', 'alt': 'GG'}, {'pos': 1210543, 'ref': '--', 'alt': 'AA'}, {'pos': 1210543, 'ref': '-', 'alt': 'A'}, {'pos': 1210544, 'ref': '-', 'alt': '-'}], 
[{'pos': 1210539, 'ref': 'G', 'alt': 'G'}, {'pos': 1210540, 'ref': '-', 'alt': 'T'}, {'pos': 1210541, 'ref': '--', 'alt': 'GG'}, {'pos': 1210541, 'ref': '--', 'alt': 'AA'}, {'pos': 1210543, 'ref': '-', 'alt': '-'}, {'pos': 1210544, 'ref': '-', 'alt': '-'}]]

i =2 æ”¹içš„æ­¥è¿›ï¼Œå¯¹äº†
diff:
[[0, 0, 1, 0, 1, 1], 
[0, 0, 0, 1, 1, 0], 
[0, 1, 0, 1, 0, 0]]
[[{'pos': 1210539, 'ref': 'G', 'alt': 'G'}, {'pos': 1210540, 'ref': '-', 'alt': '-'}, {'pos': 1210541, 'ref': '--', 'alt': 'GG'}, {'pos': 1210543, 'ref': '--', 'alt': 'AA'}, {'pos': 1210543, 'ref': '-', 'alt': 'G'}, {'pos': 1210544, 'ref': '-', 'alt': 'G'}], 
[{'pos': 1210539, 'ref': 'G', 'alt': 'G'}, {'pos': 1210540, 'ref': '-', 'alt': '-'}, {'pos': 1210541, 'ref': '--', 'alt': 'GG'}, {'pos': 1210543, 'ref': '--', 'alt': 'AA'}, {'pos': 1210543, 'ref': '-', 'alt': 'A'}, {'pos': 1210544, 'ref': '-', 'alt': '-'}], 
[{'pos': 1210539, 'ref': 'G', 'alt': 'G'}, {'pos': 1210540, 'ref': '-', 'alt': 'T'}, {'pos': 1210541, 'ref': '--', 'alt': 'GG'}, {'pos': 1210541, 'ref': '--', 'alt': 'AA'}, {'pos': 1210543, 'ref': '-', 'alt': '-'}, {'pos': 1210544, 'ref': '-', 'alt': '-'}]]

i =3
diff:
[[0, 0, 1, 0, 1, 0, 1], 
[0, 0, 0, 1, 0, 1, 0], 
[0, 1, 0, 1, 0, 0, 0, 0]]
mata:
[[{'pos': 1210539, 'ref': 'G', 'alt': 'G'}, {'pos': 1210540, 'ref': '-', 'alt': '-'}, {'pos': 1210541, 'ref': '--', 'alt': 'GG'}, {'pos': 1210543, 'ref': '--', 'alt': 'AA'}, {'pos': 1210543, 'ref': '-', 'alt': 'G'}, {'pos': 1210544, 'ref': '-', 'alt': 'A'}, {'pos': 1210544, 'ref': '-', 'alt': 'G'}], 
[{'pos': 1210539, 'ref': 'G', 'alt': 'G'}, {'pos': 1210540, 'ref': '-', 'alt': '-'}, {'pos': 1210541, 'ref': '--', 'alt': 'GG'}, {'pos': 1210543, 'ref': '--', 'alt': 'AA'}, {'pos': 1210543, 'ref': '-', 'alt': 'G'}, {'pos': 1210544, 'ref': '-', 'alt': 'A'}, {'pos': 1210544, 'ref': '-', 'alt': '-'}], 
[{'pos': 1210539, 'ref': 'G', 'alt': 'G'}, {'pos': 1210540, 'ref': '-', 'alt': 'T'}, {'pos': 1210541, 'ref': '--', 'alt': 'GG'}, {'pos': 1210541, 'ref': '--', 'alt': 'AA'}, {'pos': 1210543, 'ref': '-', 'alt': 'G'}, {'pos': 1210544, 'ref': '-', 'alt': 'A'}, {'pos': 1210543, 'ref': '-', 'alt': '-'}, {'pos': 1210544, 'ref': '-', 'alt': '-'}]]
'''
 # è¾“å…¥dna_setå’Œthresholdï¼Œè¾“å‡ºæ–°çš„set
def hamming_similarity(seq1, seq2):
    """è®¡ç®—ä¸¤ä¸ªç­‰é•¿ DNA åºåˆ—çš„ Hamming ç›¸ä¼¼åº¦"""
    assert len(seq1) == len(seq2), "åºåˆ—é•¿åº¦å¿…é¡»ç›¸åŒ"
    return sum(a == b for a, b in zip(seq1, seq2)) / len(seq1)

# ç°åœ¨æ‰€æœ‰é‡å¤çš„DNAåºåˆ—ä¹Ÿä¼šè¢«ä¿ç•™ä¸‹æ¥äº†
def filter_similar_sequences(dna_list, threshold):
    """ä¿ç•™ç›¸ä¼¼åº¦å¤§äºç­‰äº threshold çš„ DNA åºåˆ—"""
    #dna_list = list(dna_set)  # è½¬ä¸ºåˆ—è¡¨
    n = len(dna_list)
    similarity_matrix = np.zeros((n, n))

    # è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ
    for i, j in combinations(range(n), 2):
        similarity_matrix[i, j] = similarity_matrix[j, i] = hamming_similarity(dna_list[i], dna_list[j])

    # ç»Ÿè®¡æ¯ä¸ªåºåˆ—ä¸å…¶ä»–åºåˆ—çš„æœ€å¤§ç›¸ä¼¼åº¦
    max_similarities = similarity_matrix.max(axis=1)

    # åªä¿ç•™è‡³å°‘æœ‰ä¸€ä¸ªç›¸ä¼¼åº¦ â‰¥ threshold çš„åºåˆ—
    filtered_sequences = {dna_list[i] for i in range(n) if max_similarities[i] >= threshold}

    return filtered_sequences
